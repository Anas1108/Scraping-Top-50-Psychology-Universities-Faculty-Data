{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import html5lib\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://psychology.fas.harvard.edu/faculty',\n",
       " 'https://psychology.stanford.edu/people/faculty',\n",
       " 'https://www.psychol.cam.ac.uk/people/faculty',\n",
       " 'https://www.psy.ox.ac.uk/team',\n",
       " 'https://psychology.berkeley.edu/people/faculty',\n",
       " 'https://www.psych.ucla.edu/faculty',\n",
       " 'https://www.ucl.ac.uk/pals/people/academic-staff',\n",
       " 'https://psychology.yale.edu/people/faculty/primary',\n",
       " 'https://bcs.mit.edu/our-faculty',\n",
       " 'https://lsa.umich.edu/psych/people/faculty.directory.html',\n",
       " 'https://as.nyu.edu/content/nyu-as/as/departments/psychology/people/faculty.html',\n",
       " 'https://psychology.columbia.edu/content/faculty',\n",
       " 'https://psych.ubc.ca/people/',\n",
       " 'https://psychology.sas.upenn.edu/people/standing-faculty',\n",
       " 'https://www.uva.nl/en/about-the-uva/organisation/faculties/faculty-of-social-and-behavioural-sciences/disciplines/psychology/psychology-scientific-staff.html',\n",
       " 'https://psychology.uchicago.edu/directories/full/faculty',\n",
       " 'https://psych.princeton.edu/people/faculty/all',\n",
       " 'https://psychologicalsciences.unimelb.edu.au/people/academic',\n",
       " 'https://kclpure.kcl.ac.uk/portal/en/organisations/psychological-medicine(e774a2e2-f915-4fdb-871a-afa1a3766a7c)/persons.html?filter=academic']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank1_URL=\"https://psychology.fas.harvard.edu/faculty\"\n",
    "rank2_URL=\"https://psychology.stanford.edu/people/faculty\"\n",
    "rank3_URL=\"https://www.psychol.cam.ac.uk/people/faculty\"\n",
    "rank4_URL=\"https://www.psy.ox.ac.uk/team\"\n",
    "rank5_URL=\"https://psychology.berkeley.edu/people/faculty\"\n",
    "rank6_URL=\"https://www.psych.ucla.edu/faculty\"\n",
    "rank7_URL='https://www.ucl.ac.uk/pals/people/academic-staff'\n",
    "rank8_URL='https://psychology.yale.edu/people/faculty/primary'\n",
    "rank9_URL='https://bcs.mit.edu/our-faculty'\n",
    "rank10_URL='https://lsa.umich.edu/psych/people/faculty.directory.html'\n",
    "rank11_URL='https://as.nyu.edu/content/nyu-as/as/departments/psychology/people/faculty.html'\n",
    "rank12_URL='https://psychology.columbia.edu/content/faculty'\n",
    "rank13_URL='https://psych.ubc.ca/people/'\n",
    "rank14_URL='https://psychology.sas.upenn.edu/people/standing-faculty'\n",
    "rank15_URL='https://www.uva.nl/en/about-the-uva/organisation/faculties/faculty-of-social-and-behavioural-sciences/disciplines/psychology/psychology-scientific-staff.html'\n",
    "rank16_URL='https://psychology.uchicago.edu/directories/full/faculty'\n",
    "rank17_URL='https://psych.princeton.edu/people/faculty/all'\n",
    "rank18_URL='https://psychologicalsciences.unimelb.edu.au/people/academic'\n",
    "rank20_URL='https://kclpure.kcl.ac.uk/portal/en/organisations/psychological-medicine(e774a2e2-f915-4fdb-871a-afa1a3766a7c)/persons.html?filter=academic'\n",
    "university_faculty_URL=[rank1_URL,\n",
    "rank2_URL,\n",
    "rank3_URL,\n",
    "rank4_URL,\n",
    "rank5_URL,\n",
    "rank6_URL,\n",
    "rank7_URL,\n",
    "rank8_URL,\n",
    "rank9_URL,\n",
    "rank10_URL,\n",
    "rank11_URL,\n",
    "rank12_URL,\n",
    "rank13_URL,\n",
    "rank14_URL,\n",
    "rank15_URL,\n",
    "rank16_URL,\n",
    "rank17_URL,\n",
    "rank18_URL,\n",
    "rank20_URL]\n",
    "university_faculty_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def create_csv_files(rankName,dictionary):\n",
    "    filename=rankName +'.csv'\n",
    "    with open(filename, 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in dictionary.items():\n",
    "            writer.writerow([key, value])\n",
    "    df=pd.read_csv(filename,encoding='latin-1', header=None)\n",
    "    df=(df.rename(columns={0: 'Professor Name',1: 'Professor Title'}))\n",
    "    df.to_csv (filename, index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_Citation=[]\n",
    "author_HINDEX_ALL=[]\n",
    "author_I10_INDEX_ALL=[]\n",
    "author_year_list=[]\n",
    "author_year_value_list=[]\n",
    "author_googleScholar_page_url=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rank1 university scrapping\n",
    "def rank1_uni_Scrapping():  \n",
    "    cnt = requests.get(rank1_URL)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_a_Tag=[]\n",
    "    professorTitle=[]\n",
    "    First_Div=[]\n",
    "    li=[]\n",
    "    a=[]\n",
    "    rqrd_h1 = soup.find_all(\"h1\", attrs={\"class\": \"node-title\"})\n",
    "    for x in rqrd_h1:\n",
    "        rqrd_a_Tag.append(x.find(\"a\"))\n",
    "    for x in rqrd_a_Tag:\n",
    "        professorName.append(str(x.text))\n",
    "    rqrd_div = soup.find_all(\"div\", attrs={\"class\": \"field field-name-field-professional-title field-type-text field-label-hidden view-mode-teaser\"})\n",
    "    for x in rqrd_div:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    rank1_dict = {professorName[i]: professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rank2 university scrapping\n",
    "def rank2_uni_scrapping():\n",
    "    cnt = requests.get(rank2_URL)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_a_Tag=[]\n",
    "    rqrd_div = soup.find_all(\"div\", attrs={\"class\": \"postcard-col2\"})\n",
    "    for x in rqrd_div:\n",
    "            rqrd_a_Tag.append(x.find(\"a\"))\n",
    "    for x in rqrd_a_Tag:\n",
    "            professorName.append(str(x.text))\n",
    "    all_professor_a_tag=[]\n",
    "    for x in rqrd_a_Tag:\n",
    "            all_professor_a_tag.append(x.get('href'))\n",
    "    professorTitle=[]\n",
    "    def scrapp_rank2_uni_Proff_Title(url):\n",
    "        cnt = requests.get(url)\n",
    "        htmlContent = cnt.content\n",
    "        soup = BeautifulSoup(htmlContent,'html.parser')\n",
    "        li=[]\n",
    "        rqrd_div = soup.find(\"div\", attrs={\"class\": \"nameAndTitle\"})\n",
    "        h2_tag=rqrd_div.find('h2')\n",
    "        professorTitle.append(str(h2_tag.text).strip())\n",
    "    for x in range(len(all_professor_a_tag)):\n",
    "        if x==8 or x==38 or x==44 or x==50 or x==52 or x==35:\n",
    "            professorTitle.append(\"unauthorized\")\n",
    "            continue\n",
    "        print(x)\n",
    "        scrapp_rank2_uni_Proff_Title(all_professor_a_tag[x])\n",
    "    rank2_dict = {professorName[i]: professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rank3 university scrapping\n",
    "def rank3_uni_scrapping():\n",
    "    cnt = requests.get(rank3_URL)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_h3_Tag=[]\n",
    "    rqrd_div = soup.find_all(\"div\", attrs={\"class\": \"field-item even\"})\n",
    "    for x in rqrd_div:\n",
    "            rqrd_h3_Tag.append(x.find(\"h3\"))\n",
    "    for x in rqrd_h3_Tag:\n",
    "        if x==None:\n",
    "            continue\n",
    "        professorName.append(str(x.text).strip())\n",
    "    professorTitle=[]\n",
    "    First_Div=[]\n",
    "    li=[]\n",
    "    rqrd_title__div = soup.find_all(\"div\", attrs={\"class\": \"field field-name-field-sd-job-titles field-type-text field-label-hidden\"})\n",
    "    for x in rqrd_title__div:\n",
    "            li.append(x.find(\"div\"))\n",
    "    for x in li:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    rank3_dict = {professorName[i]: professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank3_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rank4 university scrapping\n",
    "def rank4_uni_scrapping():\n",
    "    cnt = requests.get(rank4_URL)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_h2_Tag=[]\n",
    "    rqrd_p_tag=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_div = soup.find_all(\"div\", attrs={\"class\": \"col-xs-8\"})\n",
    "    for x in rqrd_div:\n",
    "            rqrd_h2_Tag.append(x.find(\"h2\"))\n",
    "    for x in rqrd_h2_Tag:\n",
    "        professorName.append(str(x.text).strip())\n",
    "    for x in rqrd_div:\n",
    "            rqrd_p_tag.append(x.find(\"p\"))\n",
    "    for x in rqrd_p_tag:\n",
    "        professorTitle.append(str(x.text).strip())     \n",
    "    rank4_dict = {professorName[i]: professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank4_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rank5 university scrapping\n",
    "def rank5_uni_scrapping():\n",
    "    cnt = requests.get(rank5_URL)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_span_Tag=[]\n",
    "    rqrd_title_div_tag=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_div = soup.find_all(\"div\", attrs={\"class\": \"panel-panel panel-col\"})\n",
    "    for x in rqrd_div:\n",
    "            rqrd_span_Tag.append(x.find(\"span\"))\n",
    "    for x in rqrd_span_Tag:\n",
    "            professorName.append(str(x.text).strip())\n",
    "    rqrd_title_div_tag=soup.find_all(\"div\", attrs={\"class\": \"views-field views-field-field-admin-title\"})\n",
    "    for x in rqrd_title_div_tag:\n",
    "            professorTitle.append(str(x.text).strip())     \n",
    "    rank5_dict = {professorName[i]: professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank5_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rank7 university scrapping\n",
    "def rank7_uni_scrapping():\n",
    "    cnt = requests.get(rank7_URL)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_td_Tag=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_tr = soup.find_all(\"tr\", attrs={\"height\": \"20\"})\n",
    "    for x in rqrd_tr:\n",
    "        rqrd_td_Tag.append(x.find_all(\"td\"))\n",
    "    for i in range(len(rqrd_td_Tag)):\n",
    "        if i==0:\n",
    "            continue\n",
    "        for x in range(len(rqrd_td_Tag[i])):\n",
    "            if x==2:\n",
    "                break\n",
    "            if x==0:\n",
    "                professorName.append(str(rqrd_td_Tag[i][x].text).strip())\n",
    "            if x==1:\n",
    "                professorTitle.append(str(rqrd_td_Tag[i][x].text).strip())\n",
    "#     print(len(professorName))\n",
    "#     print(len(professorTitle))\n",
    "    #print((professorName))\n",
    "    #print((professorTitle)) \n",
    "rank7_uni_scrapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rank8 university scrapping\n",
    "def rank8_uni_scrapping():\n",
    "    cnt = requests.get(rank8_URL)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_a_Tag=[]\n",
    "    rqrd_title_br_tag=[]\n",
    "    professorTitle=[]\n",
    "    all_professor_a_tag=[]\n",
    "    li=[]\n",
    "    rqrd_td = soup.find_all(\"td\", attrs={\"class\": \"views-field views-field-name\"})\n",
    "    for x in rqrd_td:\n",
    "            rqrd_a_Tag.append(x.find(\"a\"))\n",
    "    for x in rqrd_a_Tag:\n",
    "        professorName.append(str(x.text).strip())\n",
    "    for x in rqrd_a_Tag:\n",
    "            all_professor_a_tag.append(x.get('href'))\n",
    "    def scrapp_rank8_uni_Proff_Title(url):\n",
    "        cnt = requests.get(url)\n",
    "        htmlContent = cnt.content\n",
    "        soup = BeautifulSoup(htmlContent,'html.parser')\n",
    "        rqrd_div = soup.find(\"div\", attrs={\"class\": \"field field-name-field-title field-type-text field-label-hidden\"})\n",
    "        professorTitle.append(str(rqrd_div.text).strip())\n",
    "    for x in range(len(all_professor_a_tag)):\n",
    "        scrapp_rank8_uni_Proff_Title('https://psychology.yale.edu'+all_professor_a_tag[x])\n",
    "    rank8_dict={}\n",
    "    rank8_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank8_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rank9 university scrapping\n",
    "def rank9_uni_scrapping():\n",
    "    cnt = requests.get(rank9_URL)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_a_Tag=[]\n",
    "    rqrd_title_br_tag_li=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_div = soup.find_all(\"div\", attrs={\"class\": \"field user-full-name-link\"})\n",
    "    for x in rqrd_div:\n",
    "            rqrd_a_Tag.append(x.find(\"a\"))\n",
    "    for x in rqrd_a_Tag:\n",
    "        professorName.append(str(x.text).strip())\n",
    "    rqrd_title_div_tag=soup.find_all(\"div\", attrs={\"class\": \"group-profile-info field-group-div\"})\n",
    "    for x in rqrd_title_div_tag:\n",
    "            rqrd_title_br_tag_li.append(x.find(\"div\"))\n",
    "    for x in rqrd_title_br_tag_li:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    rank9_dict = {professorName[i]: professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank9_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank10_uni_scrapping():\n",
    "    cnt=requests.get(rank10_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_a_Tag=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_div=soup.find_all(\"div\",attrs={\"class\":\"col-sm-5 col-xs-12\"})\n",
    "    for x in rqrd_div:\n",
    "        rqrd_a_Tag.append(x.find(\"a\"))\n",
    "    for x in rqrd_a_Tag:\n",
    "        professorName.append(str(x.text).strip()) \n",
    "    rqrd_title_span_tag=soup.find_all(\"span\",attrs={\"class\":\"title\"})\n",
    "    for x in rqrd_title_span_tag:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    rank10_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank10_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank11_uni_scrapping():\n",
    "    cnt=requests.get(rank11_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_a_Tag=[]\n",
    "    rqrd_title_span_tag=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_a_Tag=soup.find_all(\"a\",attrs={\"class\":\"facultydirectorybio-person__name theme__text--dark\"})\n",
    "    for x in rqrd_a_Tag:\n",
    "        professorName.append(str(x.text).strip())\n",
    "    rqrd_Title_Tag=soup.find_all(\"div\",attrs={\"class\":\"filtered-items-item js-filter-item\"})\n",
    "    for x in rqrd_Title_Tag:\n",
    "        rqrd_title_span_tag.append(x.find(\"span\"))\n",
    "#     rqrd_title_span_tag=soup.find_all(\"span\",attrs={\"class\":\"facultydirectorybio-person__position\"})\n",
    "    for x in rqrd_title_span_tag:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    rank11_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank11_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank12_uni_scrapping():\n",
    "    cnt=requests.get(rank12_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_h2_Tag=[]\n",
    "    rqrd_title_div_tag=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_article_Tag=soup.find_all(\"article\",attrs={\"class\":\"contextual-region teaser\"})\n",
    "    for x in rqrd_article_Tag:\n",
    "        rqrd_h2_Tag.append(x.find(\"h2\"))\n",
    "    for x in rqrd_h2_Tag:\n",
    "        professorName.append(str(x.text).strip())\n",
    "    for x in rqrd_article_Tag:\n",
    "        rqrd_title_div_tag.append(x.find_all(\"div\"))\n",
    "    for i in range(len(rqrd_title_div_tag)):\n",
    "        for x in range(len(rqrd_title_div_tag[i])):\n",
    "            if x==4:\n",
    "                professorTitle.append(str(rqrd_title_div_tag[i][x].text).strip())\n",
    "    rank12_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank12_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank13_uni_scrapping():\n",
    "    cnt=requests.get(rank13_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_p_Tag=[]\n",
    "    rqrd_title_br_tag_li=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_p=soup.find_all(\"p\",attrs={\"class\":\"name\"})\n",
    "    for x in rqrd_p:\n",
    "        professorName.append(str(x.text).strip())\n",
    "    rqrd_title=soup.find_all(\"p\",attrs={\"class\":\"title\"})\n",
    "    for x in rqrd_title:\n",
    "        professorTitle.append(str(x.text).strip())   \n",
    "    rank13_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank13_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank14_uni_scrapping():\n",
    "    cnt=requests.get(rank14_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_div=soup.find_all(\"div\",attrs={\"class\":\"views-field views-field-title\"})\n",
    "    for x in rqrd_div:\n",
    "        professorName.append(str(x.text).strip())\n",
    "    rqrd_title_div_tag=soup.find_all(\"div\",attrs={\"class\":\"views-field views-field-field-official-title\"})\n",
    "    for x in rqrd_title_div_tag:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    # print(professorTitle)\n",
    "    rank14_dict={}\n",
    "    rank14_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank14_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank15_uni_scrapping():\n",
    "    cnt=requests.get(rank15_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_title_span_tag_li=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_div=soup.find_all(\"div\",attrs={\"class\":\"c-listitem__title\"})\n",
    "    for x in rqrd_div:\n",
    "        professorName.append(str(x.text).strip())   \n",
    "    rqrd_title_div_tag=soup.find_all(\"p\",attrs={\"class\":\"c-listitem__contact c-listitem--link c-listitem--multiline\"})\n",
    "    for x in rqrd_title_div_tag:\n",
    "        rqrd_title_span_tag_li.append(x.find(\"span\"))\n",
    "    for x in rqrd_title_span_tag_li:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    rank15_dict={}\n",
    "    rank15_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank15_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank16_uni_scrapping():\n",
    "    cnt=requests.get(rank16_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_title_a_tag_li=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_div=soup.find_all(\"div\",attrs={\"class\":\"column-width-5\"})\n",
    "    for x in rqrd_div:\n",
    "        rqrd_title_a_tag_li.append(x.find(\"a\"))\n",
    "    for x in range(len(rqrd_title_a_tag_li)):\n",
    "        if x==0:\n",
    "            continue\n",
    "        professorName.append(str(rqrd_title_a_tag_li[x].text).strip())\n",
    "    rqrd_p_tag=soup.find_all(\"p\",attrs={\"class\":\"title\"})\n",
    "    for x in rqrd_p_tag:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    rank16_dict={}\n",
    "    rank16_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank16_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank17_uni_scrapping():\n",
    "    cnt=requests.get(rank17_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    professorTitle=[]\n",
    "    rqrd_span=soup.find_all(\"span\",attrs={\"class\":\"field-content\"})\n",
    "    for x in rqrd_span:\n",
    "        professorName.append(str(x.text).strip())  \n",
    "    rqrd_title_div_tag=soup.find_all(\"div\",attrs={\"class\":\"views-field views-field-field-psych-title\"})\n",
    "    for x in rqrd_title_div_tag:\n",
    "        professorTitle.append(str(x.text).strip())\n",
    "    rank17_dict={}\n",
    "    rank17_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank17_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b4e88da9e952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mrqrd_h1_Tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"h1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrqrd_h1_Tag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m# print(li)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mrqrd_div\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"details\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# def rank18_uni_scrapping():\n",
    "cnt=requests.get(rank18_URL)\n",
    "htmlContent=cnt.content\n",
    "soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "professorName=[]\n",
    "rqrd_a_Tag=[]\n",
    "rqrd_h1_Tag=[]\n",
    "rqrd_title_br_tag_li=[]\n",
    "professorTitle=[]\n",
    "res=[]\n",
    "rqrd_ul=soup.find_all(\"ul\",attrs={\"class\":\"staff-listing min\"})\n",
    "for x in rqrd_ul:\n",
    "    rqrd_h1_Tag.append(x.find(\"h1\"))\n",
    "for x in rqrd_h1_Tag:\n",
    "    li.append(str(x.text).strip())\n",
    "# print(li)    \n",
    "rqrd_div=soup.find_all(\"div\",attrs={\"details\"})\n",
    "for x in rqrd_div:\n",
    "    rqrd_a_Tag.append(x.find(\"a\"))\n",
    "for x in rqrd_a_Tag:\n",
    "    professorName.append(str(x.text).strip())\n",
    "print(professorName)\n",
    "# for x in professorName:\n",
    "#     res.append(x.split())\n",
    "# for x in range(len(res[i])):\n",
    "#     for k in li:\n",
    "#         print(res[x][0])\n",
    "#         if res[x]==k:\n",
    "#             professorTitle.append(k)\n",
    "# print(professorTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank20_uni_scrapping():\n",
    "    cnt=requests.get(rank20_URL)\n",
    "    htmlContent=cnt.content\n",
    "    soup=BeautifulSoup(htmlContent,'html.parser')\n",
    "    professorName=[]\n",
    "    rqrd_p_Tag=[]\n",
    "    rqrd_span_Tag=[]\n",
    "    professorTitle=[]\n",
    "    li=[]\n",
    "    rqrd_div=soup.find_all(\"div\",attrs={\"class\":\"rendering rendering_person rendering_short rendering_person_short\"})\n",
    "    for x in rqrd_div:\n",
    "        rqrd_a_Tag.append(x.find(\"a\"))\n",
    "    for x in rqrd_a_Tag:\n",
    "        rqrd_span_Tag.append(x.find(\"span\"))\n",
    "    for x in rqrd_span_Tag:\n",
    "        li.append(str(x.text).strip())  \n",
    "    for x in rqrd_div:\n",
    "        rqrd_p_Tag.append(x.find(\"p\"))\n",
    "    for x in range(len(rqrd_p_Tag)):\n",
    "        if x==9 or x==26:\n",
    "            professorTitle.append('unknown')\n",
    "            continue\n",
    "        professorTitle.append(str(rqrd_p_Tag[x].text).strip())\n",
    "    for i in range(44):\n",
    "        professorName.append(li[i])\n",
    "    rank20_dict={}\n",
    "    rank20_dict={professorName[i]:professorTitle[i] for i in range(len(professorName))}\n",
    "    return(rank20_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList(dict):\n",
    "    return list(dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url_of_author_page(name):\n",
    "    try:\n",
    "        author_data = next(scholarly.search_author(name))\n",
    "        scholar_Id=(author_data['scholar_id'])\n",
    "        url='https://scholar.google.com/citations?hl=en&user='+scholar_Id\n",
    "        author_googleScholar_page_url.append(url)\n",
    "    except Exception as e:\n",
    "        author_googleScholar_page_url.append('Not Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_author_googleScholar_data(url):\n",
    "    cnt = requests.get(url)\n",
    "    htmlContent = cnt.content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    graph_values=[]\n",
    "    graph_years=[]\n",
    "    valueName=['CITATIONS_ALL','HINDEX_ALL','I10_INDEX_ALL']\n",
    "    li=[]\n",
    "    rqrd_td=[]\n",
    "    rdrd_table=soup.find(\"table\", attrs={\"id\": \"gsc_rsb_st\"})\n",
    "    rqrd_tr = rdrd_table.find_all(\"tr\")\n",
    "    for x in range(len(rqrd_tr)):\n",
    "        if x==0:\n",
    "            continue\n",
    "        rqrd_td.append(rqrd_tr[x].find_all('td'))\n",
    "    for x in range(len(rqrd_td[1])):\n",
    "            li.append(str(rqrd_td[x][1].text).strip())\n",
    "    for x in range(len(li)):\n",
    "        if x==0:\n",
    "            author_Citation.append(li[x])\n",
    "        elif x==1:\n",
    "            author_HINDEX_ALL.append(li[x])\n",
    "        elif x==2:\n",
    "            author_I10_INDEX_ALL.append(li[x])     \n",
    "    div_tag=soup.find_all(\"div\",attrs={\"id\":\"gsc_prf_in\"})\n",
    "    a_tag=soup.find_all(\"a\",attrs={\"class\":\"gsc_g_a\"})\n",
    "    for x in a_tag:\n",
    "        graph_values.append(str(x.text).strip())\n",
    "    author_year_value_list.append(graph_values)\n",
    "    span_tag=soup.find_all(\"span\",attrs={\"class\":\"gsc_g_t\"})\n",
    "    for x in span_tag:\n",
    "        graph_years.append(str(x.text).strip())\n",
    "    author_year_list.append(graph_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_goggle_scholor_page(name,rank):\n",
    "    print(\"scraping rank\"+str(rank)+\" .............\")\n",
    "    author_Citation.clear()\n",
    "    author_HINDEX_ALL.clear()\n",
    "    author_I10_INDEX_ALL.clear()\n",
    "    author_year_list.clear()\n",
    "    author_year_value_list.clear()\n",
    "    author_googleScholar_page_url.clear()\n",
    "    Professor_Name_list=getList(name)\n",
    "    Professor_Title_list=list(name.values())\n",
    "    print(\"Extracting Urls of Author's Google scholor page.....\")\n",
    "    for i in range(len(Professor_Name_list)):\n",
    "        get_url_of_author_page(Professor_Name_list[i])\n",
    "    print(author_googleScholar_page_url)\n",
    "    print(\"Extracting Data from Google Scholor Page......\")\n",
    "    for i in range(len(author_googleScholar_page_url)):\n",
    "        if author_googleScholar_page_url[i]=='Not Found':\n",
    "            author_Citation.append(None)\n",
    "            author_HINDEX_ALL.append(None)\n",
    "            author_I10_INDEX_ALL.append(None)\n",
    "            author_year_list.append(None)\n",
    "            author_year_value_list.append(None)\n",
    "            continue\n",
    "        scrap_author_googleScholar_data(author_googleScholar_page_url[i])\n",
    "    print(\"Creating Csv File.......\")\n",
    "    Scholar_df = pd.DataFrame({'Professor_Name':Professor_Name_list,'Professor_Title':Professor_Title_list,'Author_URL':author_googleScholar_page_url,'CITATIONS_ALL':author_Citation,'HINDEX_ALL':author_HINDEX_ALL,'I10_INDEX_ALL': author_I10_INDEX_ALL})\n",
    "    Starting_year=2021\n",
    "    for i in range(2021-1979):\n",
    "        indx=str(Starting_year)\n",
    "        Scholar_df[indx]=0\n",
    "        Starting_year-=1\n",
    "    for column in Scholar_df:\n",
    "        for i in range(len(author_year_list)):\n",
    "            if author_year_list[i]==None:\n",
    "                continue\n",
    "            for j in range(len(author_year_list[i])):\n",
    "                if column==author_year_list[i][j]:\n",
    "                    columnSeriesObj = Scholar_df[column]\n",
    "                    columnSeriesObj.values[i]=author_year_value_list[i][j]\n",
    "                    break\n",
    "    from IPython.display import display\n",
    "    pd.options.display.max_columns = None\n",
    "    for column in Scholar_df:\n",
    "        flag =True\n",
    "        for i in Scholar_df[column]:\n",
    "            if i!=0:\n",
    "                flag=False\n",
    "        if flag:\n",
    "            Scholar_df.drop(column,\n",
    "      axis='columns', inplace=True)\n",
    "    print('Csv File of rank'+str(rank) +' is ready')\n",
    "    Scholar_df.to_csv(\"rank\"+str(rank)+\".csv\")\n",
    "    return(Scholar_df)\n",
    "# df=scrap_goggle_scholor_page('rank4.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Matthew K. Nock': 'Department Chair Edgar Pierce Professor of Psychology', 'George A. Alvarez': 'ProfessorDIB Committee Co-Chair', 'Mahzarin R. Banaji': 'Richard Clarke Cabot Professor of Social EthicsHarvard College Professor, 2014-2019Carol K. Pforzheimer Professor at Radcliffe, 2002-2008', 'Joshua W. Buckholtz': 'Associate Professor', 'Randy L. Buckner': 'Sosland Family Professor of Psychology and of Neuroscience', 'Alfonso Caramazza': 'Daniel and Amy Starch Professor of Psychology', 'Susan E. Carey': 'Henry A. Morss, Jr. and Elisabeth W. Morss Professor of Psychology', 'Mina Cikara': 'Associate Professor', 'Fiery Cushman': 'John L. Loeb Professor of the Social Sciences', 'Samuel J. Gershman': 'Professor', 'Daniel Gilbert': 'Edgar Pierce Professor of Psychology', 'Joshua D. Greene': 'Professor', 'Mark L. Hatzenbuehler': 'John L. Loeb Associate Professor of the Social Sciences', 'Jill M. Hooley': 'ProfessorDirector of Undergraduate Studies', 'Talia Konkle': 'Assistant ProfessorDIB Committee Co-Chair', 'Max Krasnow': 'Associate Professor', 'Ellen Langer': 'Professor', 'Patrick Mair': 'Senior Lecturer in Statistics', 'Katie A. McLaughlin': 'John L. Loeb Professor of the Social Sciences', 'Richard J. McNally': 'Professor', 'Jason P. Mitchell': 'Professor', 'Elizabeth A. Phelps': 'Pershing Square Professor of Human NeuroscienceDIB Committee Co-Chair', 'Steven Pinker': 'Johnstone Family Professor of Psychology', 'Daniel L. Schacter': 'William R. Kenan, Jr. Professor', 'Jim Sidanius': 'John Lindsley Professor of Psychology in memory of William James and of African and African American Studies', 'Jesse Snedeker': 'Professor', 'Leah H. Somerville': 'ProfessorDirector of Graduate Studies, 2019-2020', 'Elizabeth S. Spelke': 'Marshall L. Berkman Professor of Psychology', 'Tomer D. Ullman': 'Assistant Professor', 'John R. Weisz': 'Professor'}\n",
      "scraping rank1 .............\n",
      "Extracting Urls of Author's Google scholor page.....\n",
      "['Not Found', 'https://scholar.google.com/citations?hl=en&user=qU8dld4AAAAJ', 'https://scholar.google.com/citations?hl=en&user=19SjuiUAAAAJ', 'https://scholar.google.com/citations?hl=en&user=5_j8wrwAAAAJ', 'https://scholar.google.com/citations?hl=en&user=iCQoHp0AAAAJ', 'https://scholar.google.com/citations?hl=en&user=hvvrK8cAAAAJ', 'Not Found', 'https://scholar.google.com/citations?hl=en&user=cjj_oMQAAAAJ', 'https://scholar.google.com/citations?hl=en&user=vhJH2VEAAAAJ', 'Not Found', 'https://scholar.google.com/citations?hl=en&user=BMUua1kAAAAJ', 'Not Found', 'https://scholar.google.com/citations?hl=en&user=gC6HZQ0AAAAJ', 'https://scholar.google.com/citations?hl=en&user=r0WwJwYAAAAJ', 'https://scholar.google.com/citations?hl=en&user=QxV9vroAAAAJ', 'https://scholar.google.com/citations?hl=en&user=aFhZ2b0AAAAJ', 'Not Found', 'https://scholar.google.com/citations?hl=en&user=BkROZP0AAAAJ', 'https://scholar.google.com/citations?hl=en&user=nCjcWz0AAAAJ', 'Not Found', 'Not Found', 'https://scholar.google.com/citations?hl=en&user=Am34JOMAAAAJ', 'https://scholar.google.com/citations?hl=en&user=bUhVerAAAAAJ', 'Not Found', 'Not Found', 'https://scholar.google.com/citations?hl=en&user=scNoS_cAAAAJ', 'https://scholar.google.com/citations?hl=en&user=YCXQq10AAAAJ', 'Not Found', 'Not Found', 'https://scholar.google.com/citations?hl=en&user=qeEPeb0AAAAJ']\n",
      "Extracting Data from Google Scholor Page......\n",
      "Creating Csv File.......\n",
      "Csv File of rank1 is ready\n"
     ]
    }
   ],
   "source": [
    "# size=input(\"How many Universities Faculty data you want to scrap\")\n",
    "size=1\n",
    "while(True):\n",
    "    cnt=0\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank1_dict=rank1_uni_Scrapping()\n",
    "    scrap_goggle_scholor_page(rank1_dict,1)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank2_dict=rank2_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank2_dict,2)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank3_dict=rank3_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank3_dict,3)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank4_dict=rank4_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank4_dict,4)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank5_dict=rank5_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank5_dict,5)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank8_dict=rank8_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank8_dict,8)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank9_dict=rank9_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank9_dict,9)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank10_dict=rank10_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank10_dict,10)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank11_dict=rank11_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank11_dict,11)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank12_dict=rank12_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank12_dict,12)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank13_dict=rank13_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank13_dict,13)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank14_dict=rank14_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank14_dict,14)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank15_dict=rank15_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank15_dict,15)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank16_dict=rank16_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank16_dict,16)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank17_dict=rank17_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank17_dict,17)\n",
    "    if cnt==size:\n",
    "        break\n",
    "    cnt+=1\n",
    "    rank20_dict=rank20_uni_scrapping()\n",
    "    scrap_goggle_scholor_page(rank20_dict,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
